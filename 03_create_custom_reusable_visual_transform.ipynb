{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Reusable Visual Transform for AWS Glue Studio\n",
    "\n",
    "This notebook creates a custom reusable visual transform that can be used in Glue Studio to fix data quality issues with the help of an LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using database: REDSHIFT with sql dialect: PostgreSQL in region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# 3. Import necessary libraries and load environment variables\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "# loading environment variables that are stored in local file\n",
    "local_env_filename = 'dev.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "os.environ['SQL_DATABASE'] = os.getenv('SQL_DATABASE') # LOCAL, SQLALCHEMY, REDSHIFT\n",
    "os.environ['SQL_DIALECT'] = os.getenv('SQL_DIALECT') # SQlite, PostgreSQL\n",
    "os.environ['DATABASE_SECRET_NAME'] = os.getenv('DATABASE_SECRET_NAME')\n",
    "os.environ['S3_BUCKET_NAME'] = os.getenv('S3_BUCKET_NAME')\n",
    "os.environ['GLUE_IAM_ROLE_ARN'] = os.getenv('GLUE_IAM_ROLE_ARN')\n",
    "\n",
    "\n",
    "REGION = os.environ['REGION']\n",
    "SQL_DATABASE = os.environ['SQL_DATABASE']\n",
    "SQL_DIALECT = os.environ['SQL_DIALECT']\n",
    "DATABASE_SECRET_NAME = os.environ['DATABASE_SECRET_NAME']\n",
    "S3_BUCKET_NAME = os.environ['S3_BUCKET_NAME']\n",
    "GLUE_IAM_ROLE_ARN = os.environ['GLUE_IAM_ROLE_ARN']\n",
    "print(f\"Using database: {SQL_DATABASE} with sql dialect: {SQL_DIALECT} in region: {REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create component definition and save it to a json file\n",
    "json_file = {\n",
    "  \"name\": \"bedrock_dq_fix\",\n",
    "  \"displayName\": \"Fix detected data quality issue\",\n",
    "  \"description\": \"Take action to fix detected data quality issues.\",\n",
    "  \"functionName\": \"bedrock_dq_fix\",\n",
    "  \"parameters\": [\n",
    "    {\n",
    "      \"name\": \"llm\",\n",
    "      \"displayName\": \"LLM\",\n",
    "      \"description\": \"LLM that will be used to fix the data quality issue\",\n",
    "      \"isOptional\": True,\n",
    "      \"type\": \"string\",\n",
    "\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"prompt_template\",\n",
    "      \"displayName\": \"Prompt Template\",\n",
    "      \"description\": \"LLM Prompt Template that will be used to fix the data quality issue\",\n",
    "      \"isOptional\": True,\n",
    "      \"type\": \"string\",\n",
    "\n",
    "    }\n",
    "    \n",
    "  ]\n",
    "}\n",
    "\n",
    "# save the json file as bedrock-dq-fix.json\n",
    "import json\n",
    "\n",
    "# Save the JSON file\n",
    "with open('glue-component/bedrock_dq_fix.json', 'w') as f:\n",
    "    json.dump(json_file, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create implementation code for the component and save it a python file\n",
    "python_code = '''\n",
    "from awsglue import DynamicFrame\n",
    "import pyspark.sql.functions as F\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import boto3, json\n",
    "import time\n",
    "from botocore.config import Config\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import pandas as pd\n",
    "# from PIL import Image\n",
    "\n",
    "class BedrockLLMWrapper():\n",
    "    def __init__(self,\n",
    "        model_id: str = 'us.anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
    "        embedding_model_id: str = 'amazon.titan-embed-image-v1',\n",
    "        system_prompt: str = 'You are a helpful AI Assistant.',\n",
    "        region: str = 'us-east-1',\n",
    "        top_k: int = 5,\n",
    "        top_p: int = 0.7,\n",
    "        temperature: float = 0.0,\n",
    "        max_token_count: int = 4000,\n",
    "        max_attempts: int = 3,\n",
    "        debug: bool = False\n",
    "\n",
    "    ):\n",
    "\n",
    "        \n",
    "        \n",
    "        self.embedding_model_id = embedding_model_id\n",
    "        self.system_prompt = system_prompt\n",
    "        self.region = region\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.temperature = temperature\n",
    "        self.max_token_count = max_token_count\n",
    "        self.max_attempts = max_attempts\n",
    "        self.debug = debug\n",
    "        config = Config(\n",
    "            retries = {\n",
    "                'max_attempts': 10,\n",
    "                'mode': 'standard'\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", config=config, region_name=self.region)\n",
    "\n",
    "        self.model_id = model_id\n",
    "\n",
    "    def get_valid_format(self, file_format):\n",
    "        format_mapping = {\n",
    "            'jpg': 'jpeg',\n",
    "            'gif': 'gif',\n",
    "            'png': 'png',\n",
    "            'webp': 'webp'\n",
    "        }\n",
    "        return format_mapping.get(file_format.lower(), 'jpeg')  # Default to 'jpeg' if format is not recognized\n",
    "    \n",
    "    # def process_image(self, image_path, max_size=(512, 512)):\n",
    "    #     with open(image_path, \"rb\") as image_file:\n",
    "    #         # Read the image file\n",
    "    #         image = image_file.read()\n",
    "    #         image = Image.open(BytesIO(image)).convert(\"RGB\")\n",
    "            \n",
    "    #         # Resize image while maintaining aspect ratio\n",
    "    #         image.thumbnail(max_size, Image.LANCZOS)\n",
    "            \n",
    "    #         # Create a new image with the target size and paste the resized image\n",
    "    #         new_image = Image.new(\"RGB\", max_size, (255, 255, 255))\n",
    "    #         new_image.paste(image, ((max_size[0] - image.size[0]) // 2,\n",
    "    #                                 (max_size[1] - image.size[1]) // 2))\n",
    "            \n",
    "    #         # Save to BytesIO object\n",
    "    #         buffered = BytesIO()\n",
    "    #         new_image.save(buffered, format=\"JPEG\")\n",
    "            \n",
    "    #         # Encode to base64\n",
    "    #         input_image_base64 = base64.b64encode(buffered.getvalue()).decode('utf8')\n",
    "        \n",
    "    #     return input_image_base64\n",
    "\n",
    "    def get_embedding(self, input_text=None, image_path=None):\n",
    "        \"\"\"\n",
    "        This function is used to generate the embeddings for a specific chunk of text\n",
    "        \"\"\"\n",
    "        accept = 'application/json'\n",
    "        contentType = 'application/json'\n",
    "        request_body = {}\n",
    "\n",
    "        if input_text:\n",
    "            request_body[\"inputText\"] = input_text\n",
    "        if image_path:\n",
    "            # Process and encode the image\n",
    "            img_base64 = '' #self.process_image(image_path)\n",
    "            request_body[\"inputImage\"] = img_base64\n",
    "\n",
    "        # request_body[\"dimensions\"] = 1024\n",
    "        # request_body[\"normalize\"] = True\n",
    "\n",
    "        if 'amazon' in self.embedding_model_id:\n",
    "            embeddingInput = json.dumps(request_body)\n",
    "            response = self.bedrock_runtime.invoke_model(body=embeddingInput, \n",
    "                                                        modelId=self.embedding_model_id, \n",
    "                                                        accept=accept, \n",
    "                                                        contentType=contentType)\n",
    "            embeddingVector = json.loads(response['body'].read().decode('utf8'))\n",
    "            return embeddingVector['embedding']\n",
    "                \n",
    "        if 'cohere' in self.embedding_model_id:\n",
    "            request_body[\"input_type\"] = \"search_document\" # |search_query|classification|clustering\n",
    "            request_body[\"truncate\"] = \"NONE\" # NONE|START|END\n",
    "            embeddingInput = json.dumps(request_body)\n",
    "    \n",
    "            response = self.bedrock_runtime.invoke_model(body=embeddingInput, \n",
    "                                                            modelId=self.embedding_model_id, \n",
    "                                                            accept=accept, \n",
    "                                                            contentType=contentType)\n",
    "    \n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            # print(response_body)\n",
    "            embeddingVector = response_body['embedding']\n",
    "            \n",
    "            return embeddingVector\n",
    "    \n",
    "    def generate(self,prompt,attachment_file=None, image_file=None, image_file2=None):\n",
    "        if self.debug: \n",
    "            print('entered BedrockLLMWrapper generate')\n",
    "        message = {}\n",
    "        attempt = 1\n",
    "        if image_file is not None:\n",
    "            if self.debug: \n",
    "                print('processing image1: ', image_file)\n",
    "            # extract file format from the image file\n",
    "            file_format = image_file.split('.')[-1]\n",
    "            valid_format = self.get_valid_format(file_format)\n",
    "\n",
    "            # Open and read the image file\n",
    "            with open(image_file, 'rb') as img_file:\n",
    "                image_bytes = img_file.read()\n",
    "                if self.debug: \n",
    "                    print('image_bytes: ', image_bytes)\n",
    "                    print('valid_format: ', valid_format)\n",
    "\n",
    "            message = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    { \"text\": \"Image 1:\" },\n",
    "                    {\n",
    "                        \"image\": {\n",
    "                            \"format\": valid_format,\n",
    "                            \"source\": {\n",
    "                                \"bytes\": image_bytes \n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    { \"text\": prompt }\n",
    "                ],\n",
    "                    }\n",
    "            \n",
    "        if image_file is not None and image_file2 is not None:\n",
    "            if self.debug: \n",
    "                print('processing image2: ', image_file2)\n",
    "            # extract file format from the image file\n",
    "            file_format2 = image_file2.split('.')[-1]\n",
    "            valid_format2 = self.get_valid_format(file_format2)\n",
    "\n",
    "            with open(image_file2, 'rb') as img_file:\n",
    "                image_bytes2 = img_file.read()\n",
    "                if self.debug: \n",
    "                    print('image_bytes2: ', image_bytes2)\n",
    "                    print('valid_format2: ', valid_format2)\n",
    "            \n",
    "            message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \"text\": \"Image 1:\" },\n",
    "                {\n",
    "                    \"image\": {\n",
    "                        \"format\": valid_format,\n",
    "                        \"source\": {\n",
    "                            \"bytes\": image_bytes \n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                { \"text\": \"Image 2:\" },\n",
    "                {\n",
    "                    \"image\": {\n",
    "                        \"format\": valid_format2,\n",
    "                        \"source\": {\n",
    "                            \"bytes\": image_bytes2 \n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                { \"text\": prompt }\n",
    "            ],\n",
    "                }\n",
    "        \n",
    "        if attachment_file is not None:\n",
    "            with open(attachment_file, 'rb') as attachment_file:\n",
    "                attachment_bytes = attachment_file.read()\n",
    "                if self.debug: \n",
    "                    print('attachment_bytes: ', attachment_bytes)\n",
    "            \n",
    "            message = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"document\": {\n",
    "                            \"name\": \"Document 1\",\n",
    "                            \"format\": \"csv\",\n",
    "                            \"source\": {\n",
    "                                \"bytes\": attachment_bytes\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    { \"text\": prompt }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "        if image_file is None and image_file2 is None and attachment_file is None:\n",
    "            message = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"text\": prompt}]\n",
    "            }\n",
    "        messages = []\n",
    "        messages.append(message)\n",
    "        \n",
    "        # model specific inference parameters to use.\n",
    "        if \"anthropic\" in self.model_id.lower():\n",
    "            system_prompts = [{\"text\": self.system_prompt}]\n",
    "            # Base inference parameters to use.\n",
    "            inference_config = {\n",
    "                                \"temperature\": self.temperature, \n",
    "                                \"maxTokens\": self.max_token_count,\n",
    "                                \"stopSequences\": [\"\\\\n\\\\nHuman:\"],\n",
    "                                \"topP\": self.top_p,\n",
    "                            }\n",
    "            additional_model_fields = {\"top_k\": self.top_k}\n",
    "        else:\n",
    "            system_prompts = []\n",
    "            # Base inference parameters to use.\n",
    "            inference_config = {\n",
    "                                \"temperature\": self.temperature, \n",
    "                                \"maxTokens\": self.max_token_count,\n",
    "                            }\n",
    "            additional_model_fields = {\"top_k\": self.top_k}\n",
    "\n",
    "        if self.debug: \n",
    "            print('Sending: System: ',system_prompts,'Messages: ',str(messages))\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "\n",
    "                # Send the message.\n",
    "                response = self.bedrock_runtime.converse(\n",
    "                    modelId=self.model_id,\n",
    "                    messages=messages,\n",
    "                    system=system_prompts,\n",
    "                    inferenceConfig=inference_config,\n",
    "                    additionalModelRequestFields=additional_model_fields\n",
    "                )\n",
    "\n",
    "                text = response['output'].get('message').get('content')[0].get('text')\n",
    "                usage = response['usage']\n",
    "                latency = response['metrics'].get('latencyMs')\n",
    "\n",
    "                if self.debug: \n",
    "                    print(f'text: {text} ; and token usage: {usage} ; and query_time: {latency}')    \n",
    "                \n",
    "                break\n",
    "               \n",
    "            except Exception as e:\n",
    "                print(\"Error with calling Bedrock: \"+str(e))\n",
    "                attempt+=1\n",
    "                if attempt>self.max_attempts:\n",
    "                    print(\"Max attempts reached!\")\n",
    "                    result_text = str(e)\n",
    "                    break\n",
    "                else:#retry in 10 seconds\n",
    "                    print(\"retry\")\n",
    "                    time.sleep(60)\n",
    "\n",
    "        # return result_text\n",
    "        return [text,usage,latency]\n",
    "\n",
    "     # Threaded function for queue processing.\n",
    "    def thread_request(self, q, results):\n",
    "        while True:\n",
    "            try:\n",
    "                index, prompt = q.get(block=False)\n",
    "                data = self.generate(prompt)\n",
    "                results[index] = data\n",
    "            except Queue.Empty:\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f'Error with prompt: {str(e)}')\n",
    "                results[index] = str(e)\n",
    "            finally:\n",
    "                q.task_done()\n",
    "\n",
    " \n",
    "    def generate_threaded(self, prompts, attachments=None, images=None, max_workers=15):\n",
    "        \n",
    "        if images is None:\n",
    "            images = [None] * len(prompts)\n",
    "        elif len(prompts) != len(images):\n",
    "            raise ValueError(\"The number of prompts must match the number of images (or images must be None)\")\n",
    "        \n",
    "        if attachments is None:\n",
    "            attachments = [None] * len(prompts)\n",
    "        elif len(prompts) != len(attachments):\n",
    "            raise ValueError(\"The number of prompts must match the number of attachments (or attachments must be None)\")\n",
    "\n",
    "        results = [None] * len(prompts)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            \n",
    "            future_to_index = {executor.submit(self.generate, prompt, attachment_file, image_file): i \n",
    "                               for i, (prompt, attachment_file, image_file) in enumerate(zip(prompts, attachments, images))}\n",
    "            for future in as_completed(future_to_index):\n",
    "                index = future_to_index[future]\n",
    "                try:\n",
    "                    results[index] = future.result()\n",
    "                except Exception as exc:\n",
    "                    print(f'Generated an exception: {exc}')\n",
    "                    results[index] = str(exc)\n",
    "        \n",
    "        return results\n",
    "\n",
    "PROMPT_TEMPLATE_FIX =\"\"\"\n",
    "You are an expert data analyst specializing in data quality and anomaly detection. \n",
    "Your task is to analyze the below data quality anomaly detection result and fix the data quality issues row by row.\n",
    "\n",
    "Data quality result:\n",
    "{DATA_QUALITY_RESULT}\n",
    "\n",
    "Target table schema:\n",
    "{TARGET_TABLE_SCHEMA}\n",
    "\n",
    "Please analyze the data thoroughly.\n",
    "\n",
    "Return the response in the following JSON format, ensuring that all special characters\n",
    "are properly escaped and the JSON iswell-formed:\n",
    "\n",
    "[\n",
    "  {{\"column_name1\": \"column_value1\", \n",
    "  \"column_name2\": \"column_value2\", \n",
    "  \"column_name3\": \"column_value3\",\n",
    "  <all fields from  source data> \n",
    "  }},\n",
    "  {{<...>}},\n",
    "]\n",
    "\n",
    "Do not include the data quality result related columns DataQualityRulesPass, DataQualityRulesFail, DataQualityRulesSkip, DataQualityEvaluationResult.\n",
    "Only include the columns that are in the target table schema.\n",
    "Only include JSON and nothing else in the response\"\"\"\n",
    "\n",
    "\n",
    "def bedrock_dq_fix(self, llm=None, prompt_template=None):\n",
    "  if not llm:\n",
    "      # default to claude sonnet 3.5\n",
    "      llm = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "  \n",
    "  if not prompt_template:\n",
    "      # default to the prompt template for fixing data quality issues\n",
    "      prompt_template = PROMPT_TEMPLATE_FIX\n",
    "\n",
    "  rowLevelOutcomes_df = self.toDF()\n",
    "\n",
    "  # check if there are any data quality issues\n",
    "\n",
    "  # show all data\n",
    "  rowLevelOutcomes_df.show(truncate=False)\n",
    "  \n",
    "  # filter only the Passed records\n",
    "  rowLevelOutcomes_df_passed = rowLevelOutcomes_df.filter(rowLevelOutcomes_df.DataQualityEvaluationResult == \"Passed\") # Filter only the Passed records.\n",
    "  \n",
    "  # filter only the Failed records\n",
    "  rowLevelOutcomes_df_error = rowLevelOutcomes_df.filter(rowLevelOutcomes_df.DataQualityEvaluationResult == \"Failed\")\n",
    "\n",
    "  # show the failed records\n",
    "  rowLevelOutcomes_df.filter(rowLevelOutcomes_df.DataQualityEvaluationResult == \"Failed\").show(5, truncate=False) # Review the Failed records                    \n",
    "\n",
    "  if rowLevelOutcomes_df_error.count() == 0:\n",
    "    print(\"No data quality issues found\")\n",
    "    rowLevelOutcomes_final_df = DynamicFrame.fromDF(rowLevelOutcomes_df, self.glue_ctx, \"dq_bedrock_data\")\n",
    "    return rowLevelOutcomes_final_df\n",
    "  \n",
    "  # convert frame1 (dynamic frame) to a string\n",
    "  frame1_str = rowLevelOutcomes_df.toPandas().to_string()\n",
    "  print(f'Data Quality Result:{frame1_str}')\n",
    "\n",
    "  # convert frame2 (dynamic frame) to a string\n",
    "  source_reference_spark_df = self.glue_ctx.spark_session.table(\"source_reference_schema\")\n",
    "\n",
    "  # get the schema of the target table\n",
    "  target_table_schema = source_reference_spark_df.schema\n",
    "\n",
    "  frame2_str = source_reference_spark_df.toPandas().to_string()\n",
    "  print(f'Data Sample from target table:{frame2_str}')\n",
    "\n",
    "  prompt = prompt_template.format(DATA_QUALITY_RESULT=frame1_str, TARGET_TABLE_SCHEMA=frame2_str)\n",
    "  print(f'Prompt: {prompt}')\n",
    "\n",
    "  bedrock = BedrockLLMWrapper(debug=False, max_token_count=4096, model_id=llm)\n",
    "  print(f'Calling Bedrock to generate response')\n",
    "  result = bedrock.generate(prompt)\n",
    "  print(f'Got a LLM response: {result}')\n",
    "\n",
    "  # parse json result[0] to a dynamic dataframe\n",
    "  json_data = json.loads(result[0])\n",
    "  pandas_df = pd.DataFrame(json_data)\n",
    "  \n",
    "  # Get schema from input DynamicFrame\n",
    "  input_schema = self.schema()\n",
    "  print(f'Input schema: {str(input_schema)}')\n",
    "\n",
    "  spark_df = self.glue_ctx.spark_session.createDataFrame(pandas_df,schema=target_table_schema)\n",
    "  df = DynamicFrame.fromDF(spark_df, self.glue_ctx, \"dq_bedrock_data\")\n",
    "\n",
    "  # pretty print the dynamic dataframe\n",
    "  print(df.printSchema())\n",
    "\n",
    "  return df\n",
    "  \n",
    "# register the function as a method of DynamicFrame\n",
    "DynamicFrame.bedrock_dq_fix = bedrock_dq_fix'''\n",
    "\n",
    "\n",
    "\n",
    "# save the python script to a file\n",
    "with open('glue-component/bedrock_dq_fix.py', 'w') as f:\n",
    "    f.write(python_code) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy custom transform\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# get account id\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "bucket_name = f'aws-glue-assets-{account_id}-{REGION}'\n",
    "prefix = 'transforms'\n",
    "\n",
    "\n",
    "# create prefix 'transforms' in S3 bucket\n",
    "s3.put_object(Bucket=S3_BUCKET_NAME, Key=f'transforms/')\n",
    "\n",
    "# upload all files from glue-component to s3_path\n",
    "for file in os.listdir('glue-component'):\n",
    "    s3.upload_file(\n",
    "        f'glue-component/{file}',  # Local file path\n",
    "        bucket_name,               # S3 bucket name\n",
    "        f'{prefix}/{file}'         # S3 key (path in bucket)\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
