{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Glue DQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using database: REDSHIFT with sql dialect: PostgreSQL in region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# 3. Import necessary libraries and load environment variables\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "# loading environment variables that are stored in local file\n",
    "local_env_filename = 'dev.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "os.environ['SQL_DATABASE'] = os.getenv('SQL_DATABASE') # LOCAL, SQLALCHEMY, REDSHIFT\n",
    "os.environ['SQL_DIALECT'] = os.getenv('SQL_DIALECT') # SQlite, PostgreSQL\n",
    "os.environ['DATABASE_SECRET_NAME'] = os.getenv('DATABASE_SECRET_NAME')\n",
    "os.environ['S3_BUCKET_NAME'] = os.getenv('S3_BUCKET_NAME')\n",
    "os.environ['GLUE_IAM_ROLE_ARN'] = os.getenv('GLUE_IAM_ROLE_ARN')\n",
    "\n",
    "\n",
    "REGION = os.environ['REGION']\n",
    "SQL_DATABASE = os.environ['SQL_DATABASE']\n",
    "SQL_DIALECT = os.environ['SQL_DIALECT']\n",
    "DATABASE_SECRET_NAME = os.environ['DATABASE_SECRET_NAME']\n",
    "S3_BUCKET_NAME = os.environ['S3_BUCKET_NAME']\n",
    "GLUE_IAM_ROLE_ARN = os.environ['GLUE_IAM_ROLE_ARN']\n",
    "print(f\"Using database: {SQL_DATABASE} with sql dialect: {SQL_DIALECT} in region: {REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DatabaseUtil for Redshift\n",
    "\n",
    "from utils.database import DatabaseUtil\n",
    "\n",
    "db_util = DatabaseUtil(\n",
    "                sql_database= SQL_DATABASE,\n",
    "                region=REGION,\n",
    "                secret_name=DATABASE_SECRET_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  count\n",
      "0  None\n"
     ]
    }
   ],
   "source": [
    "# test Redshift connection and query table\n",
    "sql_statement = 'SELECT COUNT(*) FROM public.syn_data'\n",
    "result = db_util.run_sql(sql_statement)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded updated Glue job script to s3://felixh-demo/glue/glue-etl-llm-processing.py\n",
      "S3 script path: s3://felixh-demo/glue/glue-etl-llm-processing.py\n"
     ]
    }
   ],
   "source": [
    "# create AWS Glue Redshift ETL job script\n",
    "\n",
    "glue_job_script = f'''import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DoubleType, ShortType\n",
    "from awsgluedq.transforms import EvaluateDataQuality\n",
    "import logging\n",
    "import boto3\n",
    "import datetime\n",
    "import time\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json\n",
    "from botocore.config import Config\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize the Glue context\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "\n",
    "# Get job parameters\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'input_path', 'output_path', 'redshift_connection', 'redshift_table'])\n",
    "input_path = args['input_path']\n",
    "output_path = args['output_path']\n",
    "redshift_connection = args['redshift_connection']\n",
    "redshift_table = args['redshift_table']\n",
    "\n",
    "logger.info(f'Input path: {{input_path}}')\n",
    "logger.info(f'Redshift connection: {{redshift_connection}}')\n",
    "logger.info(f'Redshift table: {{redshift_table}}')\n",
    "\n",
    "\n",
    "# Set up the job\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "def get_redshift_connection_details(connection_name):\n",
    "    glue_client = boto3.client('glue')\n",
    "    try:\n",
    "        response = glue_client.get_connection(Name=connection_name)\n",
    "        connection_properties = response['Connection']['ConnectionProperties']\n",
    "        \n",
    "        return {{\n",
    "            'jdbc_url': connection_properties['JDBC_CONNECTION_URL'],\n",
    "            'username': connection_properties['USERNAME'],\n",
    "            'password': connection_properties['PASSWORD']\n",
    "        }}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting Redshift connection details: {{e}}\")\n",
    "        raise\n",
    "\n",
    "def get_redshift_table_schema(connection_details, table_name):\n",
    "    try:\n",
    "        # Use Spark to query Redshift\n",
    "        query = f\"\"\"\n",
    "            SELECT t.tablename, t.\\\\\\\"column\\\\\\\", t.type, c.column_default\n",
    "            FROM pg_table_def t\n",
    "            LEFT JOIN information_schema.columns c \n",
    "                ON t.\\\\\\\"column\\\\\\\" = c.column_name \n",
    "                AND t.schemaname = c.table_schema \n",
    "                AND t.tablename = c.table_name\n",
    "            WHERE t.schemaname = '{{table_name.split('.')[0]}}'\n",
    "            AND t.tablename = '{{table_name.split('.')[1]}}'\n",
    "        \"\"\"\n",
    "        df = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", connection_details['jdbc_url']) \\\n",
    "            .option(\"query\", query) \\\n",
    "            .option(\"user\", connection_details['username']) \\\n",
    "            .option(\"password\", connection_details['password']) \\\n",
    "            .option(\"driver\", \"com.amazon.redshift.jdbc42.Driver\") \\\n",
    "            .load()\n",
    "        \n",
    "        # Get column information\n",
    "        columns_info = df.select(\"column\", \"type\").collect()\n",
    "        \n",
    "        print(f'Redshfit table columns info: {{columns_info}}')\n",
    "        \n",
    "        # Create a schema based on Redshift data types\n",
    "        schema = StructType()\n",
    "        for col_info in columns_info:\n",
    "            col_name = col_info['column']\n",
    "            data_type = col_info['type']\n",
    "            if data_type == 'integer':\n",
    "                schema.add(StructField(col_name, IntegerType()))\n",
    "            elif data_type == 'bigint':\n",
    "                schema.add(StructField(col_name, LongType()))\n",
    "            elif data_type in ['double precision', 'real']:\n",
    "                schema.add(StructField(col_name, DoubleType()))\n",
    "            elif data_type == 'smallint':\n",
    "                schema.add(StructField(col_name, ShortType()))\n",
    "            else:\n",
    "                schema.add(StructField(col_name, StringType()))\n",
    "        \n",
    "        return schema, columns_info\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting Redshift table schema: {{e}}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "class BedrockLLMWrapper():\n",
    "    def __init__(self,\n",
    "        model_id: str = 'us.anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
    "        embedding_model_id: str = 'amazon.titan-embed-image-v1',\n",
    "        system_prompt: str = 'You are a helpful AI Assistant.',\n",
    "        region: str = 'us-east-1',\n",
    "        top_k: int = 5,\n",
    "        top_p: int = 0.7,\n",
    "        temperature: float = 0.0,\n",
    "        max_token_count: int = 4000,\n",
    "        max_attempts: int = 3,\n",
    "        debug: bool = False\n",
    "\n",
    "    ):\n",
    "\n",
    "        \n",
    "        \n",
    "        self.embedding_model_id = embedding_model_id\n",
    "        self.system_prompt = system_prompt\n",
    "        self.region = region\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.temperature = temperature\n",
    "        self.max_token_count = max_token_count\n",
    "        self.max_attempts = max_attempts\n",
    "        self.debug = debug\n",
    "        config = Config(\n",
    "            retries = {{\n",
    "                'max_attempts': 10,\n",
    "                'mode': 'standard'\n",
    "            }}\n",
    "        )\n",
    "\n",
    "        self.bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", config=config, region_name=self.region)\n",
    "\n",
    "        #use cross-region inference if model_id starts with us\n",
    "        self.model_id = model_id\n",
    "\n",
    "    def get_valid_format(self, file_format):\n",
    "        format_mapping = {{\n",
    "            'jpg': 'jpeg',\n",
    "            'gif': 'gif',\n",
    "            'png': 'png',\n",
    "            'webp': 'webp'\n",
    "        }}\n",
    "        return format_mapping.get(file_format.lower(), 'jpeg')  # Default to 'jpeg' if format is not recognized\n",
    "    \n",
    "    # def process_image(self, image_path, max_size=(512, 512)):\n",
    "    #     with open(image_path, \"rb\") as image_file:\n",
    "    #         # Read the image file\n",
    "    #         image = image_file.read()\n",
    "    #         image = Image.open(BytesIO(image)).convert(\"RGB\")\n",
    "            \n",
    "    #         # Resize image while maintaining aspect ratio\n",
    "    #         image.thumbnail(max_size, Image.LANCZOS)\n",
    "            \n",
    "    #         # Create a new image with the target size and paste the resized image\n",
    "    #         new_image = Image.new(\"RGB\", max_size, (255, 255, 255))\n",
    "    #         new_image.paste(image, ((max_size[0] - image.size[0]) // 2,\n",
    "    #                                 (max_size[1] - image.size[1]) // 2))\n",
    "            \n",
    "    #         # Save to BytesIO object\n",
    "    #         buffered = BytesIO()\n",
    "    #         new_image.save(buffered, format=\"JPEG\")\n",
    "            \n",
    "    #         # Encode to base64\n",
    "    #         input_image_base64 = base64.b64encode(buffered.getvalue()).decode('utf8')\n",
    "        \n",
    "    #     return input_image_base64\n",
    "\n",
    "    def get_embedding(self, input_text=None, image_path=None):\n",
    "        \"\"\"\n",
    "        This function is used to generate the embeddings for a specific chunk of text\n",
    "        \"\"\"\n",
    "        accept = 'application/json'\n",
    "        contentType = 'application/json'\n",
    "        request_body = {{}}\n",
    "\n",
    "        if input_text:\n",
    "            request_body[\"inputText\"] = input_text\n",
    "        if image_path:\n",
    "            # Process and encode the image\n",
    "            img_base64 = '' #self.process_image(image_path)\n",
    "            request_body[\"inputImage\"] = img_base64\n",
    "\n",
    "        # request_body[\"dimensions\"] = 1024\n",
    "        # request_body[\"normalize\"] = True\n",
    "\n",
    "        if 'amazon' in self.embedding_model_id:\n",
    "            embeddingInput = json.dumps(request_body)\n",
    "            response = self.bedrock_runtime.invoke_model(body=embeddingInput, \n",
    "                                                        modelId=self.embedding_model_id, \n",
    "                                                        accept=accept, \n",
    "                                                        contentType=contentType)\n",
    "            embeddingVector = json.loads(response['body'].read().decode('utf8'))\n",
    "            return embeddingVector['embedding']\n",
    "                \n",
    "        if 'cohere' in self.embedding_model_id:\n",
    "            request_body[\"input_type\"] = \"search_document\" # |search_query|classification|clustering\n",
    "            request_body[\"truncate\"] = \"NONE\" # NONE|START|END\n",
    "            embeddingInput = json.dumps(request_body)\n",
    "    \n",
    "            response = self.bedrock_runtime.invoke_model(body=embeddingInput, \n",
    "                                                            modelId=self.embedding_model_id, \n",
    "                                                            accept=accept, \n",
    "                                                            contentType=contentType)\n",
    "    \n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            # print(response_body)\n",
    "            embeddingVector = response_body['embedding']\n",
    "            \n",
    "            return embeddingVector\n",
    "    \n",
    "    def generate(self,prompt,attachment_file=None, image_file=None, image_file2=None):\n",
    "        if self.debug: \n",
    "            print('entered BedrockLLMWrapper generate')\n",
    "        message = {{}}    \n",
    "        attempt = 1\n",
    "        if image_file is not None:\n",
    "            if self.debug: \n",
    "                print('processing image1: ', image_file)\n",
    "            # extract file format from the image file\n",
    "            file_format = image_file.split('.')[-1]\n",
    "            valid_format = self.get_valid_format(file_format)\n",
    "\n",
    "            # Open and read the image file\n",
    "            with open(image_file, 'rb') as img_file:\n",
    "                image_bytes = img_file.read()\n",
    "                if self.debug: \n",
    "                    print('image_bytes: ', image_bytes)\n",
    "                    print('valid_format: ', valid_format)\n",
    "\n",
    "            message = {{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {{ \"text\": \"Image 1:\" }},\n",
    "                    {{\n",
    "                        \"image\": {{\n",
    "                            \"format\": valid_format,\n",
    "                            \"source\": {{\n",
    "                                \"bytes\": image_bytes \n",
    "                            }}\n",
    "                        }}\n",
    "                    }},\n",
    "                    {{ \"text\": prompt }}\n",
    "                ],\n",
    "                    }}\n",
    "            \n",
    "        if image_file is not None and image_file2 is not None:\n",
    "            if self.debug: \n",
    "                print('processing image2: ', image_file2)\n",
    "            # extract file format from the image file\n",
    "            file_format2 = image_file2.split('.')[-1]\n",
    "            valid_format2 = self.get_valid_format(file_format2)\n",
    "\n",
    "            with open(image_file2, 'rb') as img_file:\n",
    "                image_bytes2 = img_file.read()\n",
    "                if self.debug: \n",
    "                    print('image_bytes2: ', image_bytes2)\n",
    "                    print('valid_format2: ', valid_format2)\n",
    "            \n",
    "            message = {{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {{ \"text\": \"Image 1:\" }},\n",
    "                {{\n",
    "                    \"image\": {{\n",
    "                        \"format\": valid_format,\n",
    "                        \"source\": {{\n",
    "                            \"bytes\": image_bytes \n",
    "                        }}\n",
    "                    }}\n",
    "                }},\n",
    "                {{ \"text\": \"Image 2:\" }},\n",
    "                {{\n",
    "                    \"image\": {{\n",
    "                        \"format\": valid_format2,\n",
    "                        \"source\": {{\n",
    "                            \"bytes\": image_bytes2 \n",
    "                        }}\n",
    "                    }}\n",
    "                }},\n",
    "                {{ \"text\": prompt }}\n",
    "            ],\n",
    "                }}\n",
    "        \n",
    "        if attachment_file is not None:\n",
    "            with open(attachment_file, 'rb') as attachment_file:\n",
    "                attachment_bytes = attachment_file.read()\n",
    "                if self.debug: \n",
    "                    print('attachment_bytes: ', attachment_bytes)\n",
    "            \n",
    "            message = {{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {{\n",
    "                        \"document\": {{\n",
    "                            \"name\": \"Document 1\",\n",
    "                            \"format\": \"csv\",\n",
    "                            \"source\": {{\n",
    "                                \"bytes\": attachment_bytes\n",
    "                            }}\n",
    "                        }}\n",
    "                    }},\n",
    "                    {{ \"text\": prompt }}\n",
    "                ]\n",
    "            }}\n",
    "            \n",
    "        if image_file is None and image_file2 is None and attachment_file is None:\n",
    "            message = {{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{{\"text\": prompt}}]\n",
    "            }}\n",
    "        messages = []\n",
    "        messages.append(message)\n",
    "        \n",
    "        # model specific inference parameters to use.\n",
    "        if \"anthropic\" in self.model_id.lower():\n",
    "            system_prompts = [{{\"text\": self.system_prompt}}]\n",
    "            # Base inference parameters to use.\n",
    "            inference_config = {{\n",
    "                                \"temperature\": self.temperature, \n",
    "                                \"maxTokens\": self.max_token_count,\n",
    "                                \"stopSequences\": [\"\\\\n\\\\nHuman:\"],\n",
    "                                \"topP\": self.top_p,\n",
    "                            }}\n",
    "            additional_model_fields = {{\"top_k\": self.top_k}}\n",
    "        else:\n",
    "            system_prompts = []\n",
    "            # Base inference parameters to use.\n",
    "            inference_config = {{\n",
    "                                \"temperature\": self.temperature, \n",
    "                                \"maxTokens\": self.max_token_count,\n",
    "                            }}\n",
    "            additional_model_fields = {{\"top_k\": self.top_k}}\n",
    "\n",
    "        if self.debug: \n",
    "            print('Sending: System: ',system_prompts,'Messages: ',str(messages))\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "\n",
    "                # Send the message.\n",
    "                response = self.bedrock_runtime.converse(\n",
    "                    modelId=self.model_id,\n",
    "                    messages=messages,\n",
    "                    system=system_prompts,\n",
    "                    inferenceConfig=inference_config,\n",
    "                    additionalModelRequestFields=additional_model_fields\n",
    "                )\n",
    "\n",
    "                text = response['output'].get('message').get('content')[0].get('text')\n",
    "                usage = response['usage']\n",
    "                latency = response['metrics'].get('latencyMs')\n",
    "\n",
    "                if self.debug: \n",
    "                    print(f'text: {{text}} ; and token usage: {{usage}} ; and query_time: {{latency}}')    \n",
    "                \n",
    "                break\n",
    "               \n",
    "            except Exception as e:\n",
    "                print(\"Error with calling Bedrock: \"+str(e))\n",
    "                attempt+=1\n",
    "                if attempt>self.max_attempts:\n",
    "                    print(\"Max attempts reached!\")\n",
    "                    result_text = str(e)\n",
    "                    break\n",
    "                else:#retry in 10 seconds\n",
    "                    print(\"retry\")\n",
    "                    time.sleep(60)\n",
    "\n",
    "        # return result_text\n",
    "        return [text,usage,latency]\n",
    "\n",
    "    # Threaded function for queue processing\n",
    "    def thread_request(self, q, results):\n",
    "        while True:\n",
    "            try:\n",
    "                index, prompt = q.get(block=False)\n",
    "                data = self.generate(prompt)\n",
    "                results[index] = data\n",
    "            except Queue.Empty:\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f'Error with prompt: {{str(e)}}')\n",
    "                results[index] = str(e)\n",
    "            finally:\n",
    "                q.task_done()\n",
    "\n",
    " \n",
    "    def generate_threaded(self, prompts, attachments=None, images=None, max_workers=15):\n",
    "        \n",
    "        if images is None:\n",
    "            images = [None] * len(prompts)\n",
    "        elif len(prompts) != len(images):\n",
    "            raise ValueError(\"The number of prompts must match the number of images (or images must be None)\")\n",
    "        \n",
    "        if attachments is None:\n",
    "            attachments = [None] * len(prompts)\n",
    "        elif len(prompts) != len(attachments):\n",
    "            raise ValueError(\"The number of prompts must match the number of attachments (or attachments must be None)\")\n",
    "\n",
    "        results = [None] * len(prompts)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            \n",
    "            future_to_index = {{executor.submit(self.generate, prompt, attachment_file, image_file): i \n",
    "                               for i, (prompt, attachment_file, image_file) in enumerate(zip(prompts, attachments, images))}}\n",
    "            for future in as_completed(future_to_index):\n",
    "                index = future_to_index[future]\n",
    "                try:\n",
    "                    results[index] = future.result()\n",
    "                except Exception as exc:\n",
    "                    print(f'Generated an exception: {{exc}}')\n",
    "                    results[index] = str(exc)\n",
    "        \n",
    "        return results\n",
    "\n",
    "PROMPT_TEMPLATE_FIX = \"\"\"\n",
    "You are an expert data analyst specializing in data quality and anomaly detection. \n",
    "Your task is to analyze the below data quality anomaly detection result and fix the data quality issues row by row.\n",
    "\n",
    "Data quality result:\n",
    "{{DATA_QUALITY_RESULT}}\n",
    "\n",
    "Target table schema:\n",
    "{{TARGET_TABLE_SCHEMA}}\n",
    "\n",
    "Please analyze the data thoroughly.\n",
    "\n",
    "Return the response in the following JSON format, ensuring that all special characters\n",
    "are properly escaped and the JSON iswell-formed:\n",
    "\n",
    "[\n",
    "  {{{{\"column_name1\": \"column_value1\", \n",
    "  \"column_name2\": \"column_value2\", \n",
    "  \"column_name3\": \"column_value3\",\n",
    "  <all fields from  source data> \n",
    "  }}}},\n",
    "  {{{{\"<...>\"}}}},\n",
    "]\n",
    "\n",
    "Do not include the data quality result related columns DataQualityRulesPass, DataQualityRulesFail, DataQualityRulesSkip, DataQualityEvaluationResult.\n",
    "Only include the columns that are in the target table schema.\n",
    "Only include JSON and nothing else in the response\"\"\"\n",
    "\n",
    "def bedrock_dq_fix(dq_results_df, redshift_schema, llm=None, prompt_template=None):\n",
    "    \n",
    "    if not llm:\n",
    "        # default to claude sonnet 3.5\n",
    "        llm = \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\"\n",
    "\n",
    "    if not prompt_template:\n",
    "        # default to the prompt template for fixing data quality issues\n",
    "        prompt_template = PROMPT_TEMPLATE_FIX\n",
    "\n",
    "    # convert dataframe to a string\n",
    "    dataframe_str = dq_results_df.to_string()\n",
    "    print(f'Data Quality Result:{{dataframe_str}}')\n",
    "\n",
    "    redshift_schema_str = str(redshift_schema)\n",
    "    print(f'Target Table Schema:{{redshift_schema_str}}')\n",
    "\n",
    "    prompt = prompt_template.format(DATA_QUALITY_RESULT=dataframe_str, TARGET_TABLE_SCHEMA=redshift_schema_str)    \n",
    "    bedrock = BedrockLLMWrapper(debug=False, max_token_count=4096, model_id=llm)\n",
    "    print(f'Calling Bedrock to generate response for prompt:{{prompt}}')\n",
    "    result = bedrock.generate(prompt)\n",
    "    print(f'Got a Bedrock response: {{str(result[0])}}')\n",
    "    # parse json result[0] to a pandas dataframe\n",
    "    json_data = json.loads(result[0])\n",
    "    print(f'Parsed JSON data to pandas dataframe')\n",
    "\n",
    "    dtype_mapping = {{}}\n",
    "    for field in redshift_schema:\n",
    "        col_name = field['column']\n",
    "        data_type = field['type'].lower()\n",
    "        \n",
    "        if col_name == \"timestamp\":  # Special handling for timestamp\n",
    "            dtype_mapping[col_name] = 'int64'  # Use int64 for large timestamp values\n",
    "        elif data_type == 'integer':\n",
    "            dtype_mapping[col_name] = 'Int64'  # Using nullable integer type\n",
    "        elif data_type == 'bigint':\n",
    "            dtype_mapping[col_name] = 'int64'  # Use int64 for large integers\n",
    "        elif data_type in ['double precision', 'real']:\n",
    "            dtype_mapping[col_name] = 'float64'\n",
    "        else:\n",
    "            dtype_mapping[col_name] = 'object'  # String/text data\n",
    "\n",
    "    # Create DataFrame with explicit data types\n",
    "    updated_df = pd.DataFrame(json_data).astype(dtype_mapping)\n",
    "    print(f'Updated data in PandasDataFrame:{{updated_df.to_string()}}')\n",
    "\n",
    "    return updated_df\n",
    "\n",
    "try:\n",
    "    # Get Redshift connection details\n",
    "    connection_details = get_redshift_connection_details(redshift_connection)\n",
    "    \n",
    "    # Get Redshift table schema\n",
    "    spark_schema, redshift_schema = get_redshift_table_schema(connection_details, redshift_table)\n",
    "    logger.info(f\"Redshift table schema: {{redshift_schema}}\")\n",
    "    logger.info(f\"spark schema: {{spark_schema}}\")\n",
    "\n",
    "    # Read the CSV file from S3\n",
    "    logger.info(f\"Reading data from {{input_path}}\")\n",
    "    if not input_path.startswith('s3://'):\n",
    "        raise ValueError(f\"Invalid S3 path: {{input_path}}. Path must start with 's3://'\")\n",
    "    df = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "    \n",
    "    logger.info(f\"Read {{df.count()}} records from S3\")\n",
    "\n",
    "    # AWS GLUE DQ CHECKS\n",
    "    # https://docs.aws.amazon.com/glue/latest/dg/data-quality-gs-studio-notebooks.html\n",
    "\n",
    "    EvaluateDataQuality_ruleset = \"\"\"\n",
    "        Rules = [\n",
    "            ColumnDataType \"id\" = \"Integer\",\n",
    "            ColumnDataType \"version\" = \"Integer\",\n",
    "            ColumnLength \"name\" <= 20\n",
    "            \n",
    "        ]\n",
    "        Analyzers = [\n",
    "        RowCount ,\n",
    "        ColumnCount ,\n",
    "        ColumnLength \"name\",\n",
    "        Completeness \"id\",\n",
    "        Completeness \"version\"\n",
    "        ]\n",
    "    \"\"\"\n",
    "\n",
    "    # convert dataframe to dynamic frame\n",
    "    dyf = DynamicFrame.fromDF(df, glueContext, \"dynamic_frame\")\n",
    "\n",
    "    EvaluateDataQualityMultiframe = EvaluateDataQuality().process_rows(\n",
    "    frame=dyf,\n",
    "    ruleset=EvaluateDataQuality_ruleset,\n",
    "    publishing_options={{\n",
    "            \"dataQualityEvaluationContext\": \"EvaluateDataQualityMultiframe\",\n",
    "            \"enableDataQualityCloudWatchMetrics\": False,\n",
    "            \"enableDataQualityResultsPublishing\": False\n",
    "        }},\n",
    "        additional_options={{\"performanceTuning.caching\": \"CACHE_NOTHING\"}}\n",
    "    )\n",
    "\n",
    "    # review results\n",
    "    ruleOutcomes = SelectFromCollection.apply(\n",
    "        dfc=EvaluateDataQualityMultiframe,\n",
    "        key=\"ruleOutcomes\",\n",
    "        transformation_ctx=\"ruleOutcomes\",\n",
    "    )\n",
    "\n",
    "    ruleOutcomes.toDF().show(truncate=False)\n",
    "\n",
    "    # review row level results\n",
    "    rowLevelOutcomes = SelectFromCollection.apply(\n",
    "        dfc=EvaluateDataQualityMultiframe,\n",
    "        key=\"rowLevelOutcomes\",\n",
    "        transformation_ctx=\"rowLevelOutcomes\",\n",
    "    )\n",
    "\n",
    "    rowLevelOutcomes_df = rowLevelOutcomes.toDF() # Convert Glue DynamicFrame to SparkSQL DataFrame\n",
    "    rowLevelOutcomes_df_passed = rowLevelOutcomes_df.filter(rowLevelOutcomes_df.DataQualityEvaluationResult == \"Passed\") # Filter only the Passed records.\n",
    "    rowLevelOutcomes_df.filter(rowLevelOutcomes_df.DataQualityEvaluationResult == \"Failed\").show(5, truncate=False) # Review the Failed records                    \n",
    "    rowLevelOutcomes_df_error = rowLevelOutcomes_df.filter(rowLevelOutcomes_df.DataQualityEvaluationResult == \"Failed\")\n",
    "\n",
    "\n",
    "    # convert back to DynamicFrame before writing\n",
    "    rowLevelOutcomes_dyf_passed = DynamicFrame.fromDF(rowLevelOutcomes_df_passed, glueContext, \"passed_records\")\n",
    "    rowLevelOutcomes_dyf_error = DynamicFrame.fromDF(rowLevelOutcomes_df_error, glueContext, \"error_records\")\n",
    "    \n",
    "    # write error records to S3 destination for review\n",
    "    glueContext.write_dynamic_frame.from_options(\n",
    "        frame = rowLevelOutcomes_dyf_error,\n",
    "        connection_type = \"s3\",\n",
    "        connection_options = {{\"path\": f'{{output_path}}/etl_detected_dq_errors'}},\n",
    "        format = \"json\")\n",
    "\n",
    "    # get the target table columns from the spark schema\n",
    "    original_columns = [field.name for field in spark_schema.fields]\n",
    "         \n",
    "    # flatten the passed records\n",
    "    rowLevelOutcomes_df_passed_flat = rowLevelOutcomes_df_passed.select(original_columns)\n",
    "\n",
    "    # also attempt to fix the data quality issues if rowLevelOutcomes_df_error is not empty\n",
    "    if rowLevelOutcomes_df_error.count() == 0:\n",
    "        print(f'No data quality issues found')\n",
    "\n",
    "        df = rowLevelOutcomes_df_passed_flat\n",
    "\n",
    "    else:\n",
    "        # check type of rowLevelOutcomes_df_error\n",
    "        print(f'rowLevelOutcomes_df_error type: {{type(rowLevelOutcomes_df_error)}}')\n",
    "        # convert to pandas dataframe\n",
    "        rowLevelOutcomes_df_error_pandas = rowLevelOutcomes_df_error.toPandas()\n",
    "        print(f'Attempting to fix {{rowLevelOutcomes_df_error.count()}} data quality issues')\n",
    "        updated_df = bedrock_dq_fix(rowLevelOutcomes_df_error_pandas, redshift_schema)\n",
    "\n",
    "        # flatten the updated records (assuming updated_df is a Pandas DataFrame from bedrock_dq_fix)\n",
    "        print(f'Flattening the updated records')\n",
    "        updated_df_spark = spark.createDataFrame(updated_df)\n",
    "        updated_df_flat = updated_df_spark.select(original_columns)\n",
    "\n",
    "        # combine the two Spark DataFrames\n",
    "        print(f'Combining the two Spark DataFrames')\n",
    "        combined_df = rowLevelOutcomes_df_passed_flat.union(updated_df_flat)\n",
    "\n",
    "        # combined_df is a Spark DataFrame ready for further processing\n",
    "        print(f'Returning combined Spark DataFrame')\n",
    "        df = combined_df\n",
    "\n",
    "    # align df schema with Redshift schema\n",
    "    for field in spark_schema.fields:\n",
    "        if field.name in df.columns:\n",
    "            if isinstance(field.dataType, (IntegerType, LongType, ShortType)):\n",
    "                df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "            elif isinstance(field.dataType, DoubleType):\n",
    "                df = df.withColumn(field.name, col(field.name).cast(DoubleType()))\n",
    "            else:\n",
    "                df = df.withColumn(field.name, col(field.name).cast(StringType()))\n",
    "        else:\n",
    "            df = df.withColumn(field.name, lit(None).cast(field.dataType))\n",
    "\n",
    "    # handle null values (optional, adjust as needed)\n",
    "    for column in df.columns:\n",
    "        df = df.withColumn(column, when(col(column) == \"\", None).otherwise(col(column)))\n",
    "    \n",
    "    # convert back from Spark DataFrame to Glue DynamicFrame with only the original columns\n",
    "    dynamic_frame = DynamicFrame.fromDF(df, glueContext, \"flattened_passed_and_updated_records\")\n",
    "\n",
    "    # Write to Redshift\n",
    "    logger.info(f\"Writing data to Redshift table {{redshift_table}}\")\n",
    "    glueContext.write_dynamic_frame.from_jdbc_conf(\n",
    "        frame=dynamic_frame,\n",
    "        catalog_connection=redshift_connection,\n",
    "        connection_options={{\n",
    "            \"dbtable\": redshift_table,\n",
    "            \"database\": connection_details['jdbc_url'].split('/')[-1]\n",
    "        }},\n",
    "        redshift_tmp_dir=f\"s3://{S3_BUCKET_NAME}/redshift-tmp/\",\n",
    "        transformation_ctx=\"datasink\"\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Data successfully written to Redshift\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred: {{str(e)}}\")\n",
    "    raise\n",
    "\n",
    "# Commit the job\n",
    "job.commit()\n",
    "'''\n",
    "\n",
    "# Upload the updated script to S3\n",
    "s3_client = boto3.client('s3')\n",
    "object_key = 'glue/glue-etl-llm-processing.py'\n",
    "\n",
    "try:\n",
    "    s3_client.put_object(Bucket=S3_BUCKET_NAME, Key=object_key, Body=glue_job_script)\n",
    "    print(f\"Successfully uploaded updated Glue job script to s3://{S3_BUCKET_NAME}/{object_key}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading Glue job script to S3: {e}\")\n",
    "\n",
    "s3_script_path = f's3://{S3_BUCKET_NAME}/{object_key}'\n",
    "print(f\"S3 script path: {s3_script_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client = boto3.client('glue', region_name=REGION)\n",
    "\n",
    "glue_job_name = 'RedshiftETL-DQ-bedrock'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'RedshiftETL-DQ-bedrock', 'ResponseMetadata': {'RequestId': 'c1dc2aa4-fbfe-4bca-8a4a-765a1ecf8bc6', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 14 Nov 2024 17:26:45 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '33', 'connection': 'keep-alive', 'x-amzn-requestid': 'c1dc2aa4-fbfe-4bca-8a4a-765a1ecf8bc6'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# create AWS Glue job via boto3\n",
    " \n",
    "response = glue_client.create_job(\n",
    "    Name=glue_job_name,\n",
    "    Description='ETL job to load data from S3 to Redshift with LLM auto-correction',\n",
    "    Role=GLUE_IAM_ROLE_ARN,\n",
    "    ExecutionProperty={\n",
    "        'MaxConcurrentRuns': 1\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        # required for Amazon Bedrock use this if you have internet access\n",
    "        #'--additional-python-modules': 'boto3>=1.35.59', \n",
    "        # required for Amazon Bedrock use this if you do not have internet access\n",
    "        '--additional-python-modules': 's3://felixh-demo/lib/boto3-1.35.60-py3-none-any.whl,s3://felixh-demo/lib/botocore-1.35.60-py3-none-any.whl,s3://felixh-demo/lib/jmespath-1.0.1-py3-none-any.whl,s3://felixh-demo/lib/python_dateutil-2.9.0.post0-py2.py3-none-any.whl,s3://felixh-demo/lib/s3transfer-0.10.3-py3-none-any.whl,s3://felixh-demo/lib/six-1.16.0-py2.py3-none-any.whl,s3://felixh-demo/lib/urllib3-1.26.20-py2.py3-none-any.whl',\n",
    "        '--python-modules-installer-option': '--upgrade',\n",
    "        '--connection-names': 'dev-redshift-connection'\n",
    "    },\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': s3_script_path,\n",
    "        'PythonVersion': '3'\n",
    "    },\n",
    "    MaxRetries=0,\n",
    "    Timeout=1440,\n",
    "    Tags={\n",
    "        'usecase': 'Glue ETL DQ'\n",
    "    },\n",
    "    GlueVersion='4.0',\n",
    "    NumberOfWorkers=1,\n",
    "    WorkerType='Standard',\n",
    "    Connections={\n",
    "        'Connections': ['dev-redshift-connection']\n",
    "    }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job run ID: jr_8370337001b52848fc6dd622f6fb0c9737e7cfa988fcf4403d578fcc1fe7915d\n"
     ]
    }
   ],
   "source": [
    "# run AWS Glue job with arguments\n",
    "args = {\n",
    "    '--JOB_NAME': glue_job_name,\n",
    "    '--input_path': f's3://felixh-demo/etl-data/data_anomaly0.csv',\n",
    "    '--output_path': f's3://felixh-demo/',\n",
    "    '--redshift_connection': 'dev-redshift-connection',\n",
    "    '--redshift_table': 'public.syn_data'\n",
    "}\n",
    "\n",
    "response = glue_client.start_job_run(\n",
    "    JobName=glue_job_name,\n",
    "    Arguments=args\n",
    ")\n",
    "job_run_id = response['JobRunId']\n",
    "print(f'Job run ID: {job_run_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait on the status of the Glue job\n",
    "import time\n",
    "\n",
    "def wait_for_job_completion(job_name, run_id, max_attempts=30):\n",
    "    \"\"\"Wait for an AWS Glue job to complete, checking status every 30 seconds.\"\"\"\n",
    "    for i in range(max_attempts):\n",
    "        response = glue_client.get_job_run(JobName=job_name, RunId=run_id)\n",
    "        status = response['JobRun']['JobRunState']\n",
    "        \n",
    "        print(f\"Job status: {status} (attempt {i+1}/{max_attempts})\")\n",
    "        \n",
    "        if status in ['SUCCEEDED', 'FAILED', 'STOPPED', 'TIMEOUT']:\n",
    "            return status\n",
    "            \n",
    "        time.sleep(30)\n",
    "    \n",
    "    return 'TIMEOUT'\n",
    "\n",
    "# Wait for job completion\n",
    "final_status = wait_for_job_completion(glue_job_name, job_run_id)\n",
    "print(f\"Final job status: {final_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id      timestamp           name  version\n",
      "0   1  1683849600000     John Smith        5\n",
      "1   2  1683849660000  Sarah Johnson        6\n",
      "2   3  1683849720000     Mike Brown        5\n",
      "3   4  1683849780000    Emily Davis        7\n",
      "4   5  1683849840000  Robert Wilson        5\n"
     ]
    }
   ],
   "source": [
    "# this last job should have loaded all records to Redshift \n",
    "\n",
    "# so let'scheck if any records were loaded to Redshift\n",
    "sql_statement = 'SELECT * FROM public.syn_data'\n",
    "result = db_util.run_sql(sql_statement)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job run ID: jr_187d814e96cdeedec7f735217fa60019cfce1754d0e813056a1265114d5b03b2\n"
     ]
    }
   ],
   "source": [
    "# run AWS Glue job again with different input file that has anomalies\n",
    "args = {\n",
    "    '--JOB_NAME': glue_job_name,\n",
    "    '--input_path': f's3://felixh-demo/etl-data/data_anomaly2.csv',\n",
    "    '--output_path': f's3://felixh-demo/',\n",
    "    '--redshift_connection': 'dev-redshift-connection',\n",
    "    '--redshift_table': 'public.syn_data'\n",
    "}\n",
    "\n",
    "response = glue_client.start_job_run(\n",
    "    JobName=glue_job_name,\n",
    "    Arguments=args\n",
    ")\n",
    "job_run_id = response['JobRunId']\n",
    "print(f'Job run ID: {job_run_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: WAITING (attempt 1/30)\n",
      "Job status: RUNNING (attempt 2/30)\n",
      "Job status: RUNNING (attempt 3/30)\n",
      "Job status: RUNNING (attempt 4/30)\n",
      "Job status: RUNNING (attempt 5/30)\n",
      "Job status: RUNNING (attempt 6/30)\n",
      "Job status: SUCCEEDED (attempt 7/30)\n",
      "Final job status: SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "# Wait for job completion\n",
    "final_status = wait_for_job_completion(glue_job_name, job_run_id)\n",
    "print(f\"Final job status: {final_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [id, timestamp, name, version]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# this last job should have loaded all records to Redshift \n",
    "# despite the original data quality issues\n",
    "\n",
    "# so let'scheck if any records were loaded to Redshift\n",
    "sql_statement = 'SELECT * FROM public.syn_data'\n",
    "result = db_util.run_sql(sql_statement)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook demonstrates how to use AWS Glue Data Quality to check for data quality issues in a CSV file and then use an LLM to auto-correct the issues and load the updated records into an Amazon Redshift table."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
