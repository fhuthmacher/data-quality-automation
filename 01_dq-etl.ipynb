{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Glue DQ\n",
    "\n",
    "This notebook creates a table in Redshift, along with an AWS Glue ETL job that includes data quality checks and anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a python environment\n",
    "\n",
    "# !conda create -y --name text-to-sql python=3.11.8\n",
    "# !conda init && activate text-to-sql\n",
    "# !conda install -n text-to-sql ipykernel --update-deps --force-reinstall -y\n",
    "# !conda install -c conda-forge ipython-sql\n",
    "\n",
    "## OR\n",
    "# !python3 -m venv venv\n",
    "# !source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
    "\n",
    "# install ipykernel, which consists of IPython as well\n",
    "# !pip install ipykernel\n",
    "# create a kernel that can be used to run notebook commands inside the virtual environment\n",
    "# !python3 -m ipykernel install --user --name=venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Install dependencies\n",
    "\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using database: REDSHIFT with sql dialect: PostgreSQL in region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# 3. Import necessary libraries and load environment variables\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "# loading environment variables that are stored in local file\n",
    "local_env_filename = 'dev.env'\n",
    "load_dotenv(find_dotenv(local_env_filename),override=True)\n",
    "\n",
    "os.environ['REGION'] = os.getenv('REGION')\n",
    "os.environ['SQL_DATABASE'] = os.getenv('SQL_DATABASE') # LOCAL, SQLALCHEMY, REDSHIFT\n",
    "os.environ['SQL_DIALECT'] = os.getenv('SQL_DIALECT') # SQlite, PostgreSQL\n",
    "os.environ['DATABASE_SECRET_NAME'] = os.getenv('DATABASE_SECRET_NAME')\n",
    "os.environ['S3_BUCKET_NAME'] = os.getenv('S3_BUCKET_NAME')\n",
    "os.environ['GLUE_IAM_ROLE_ARN'] = os.getenv('GLUE_IAM_ROLE_ARN')\n",
    "\n",
    "\n",
    "REGION = os.environ['REGION']\n",
    "SQL_DATABASE = os.environ['SQL_DATABASE']\n",
    "SQL_DIALECT = os.environ['SQL_DIALECT']\n",
    "DATABASE_SECRET_NAME = os.environ['DATABASE_SECRET_NAME']\n",
    "S3_BUCKET_NAME = os.environ['S3_BUCKET_NAME']\n",
    "GLUE_IAM_ROLE_ARN = os.environ['GLUE_IAM_ROLE_ARN']\n",
    "print(f\"Using database: {SQL_DATABASE} with sql dialect: {SQL_DIALECT} in region: {REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DatabaseUtil for Redshift\n",
    "\n",
    "from utils.database import DatabaseUtil\n",
    "\n",
    "db_util = DatabaseUtil(\n",
    "                sql_database= SQL_DATABASE,\n",
    "                region=REGION,\n",
    "                secret_name=DATABASE_SECRET_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL execution completed.\n"
     ]
    }
   ],
   "source": [
    "# Create table in Redshift (if table does not exist)\n",
    "\n",
    "sql_statements = ['''CREATE TABLE public.syn_data\n",
    "(\n",
    "    id integer,\n",
    "    timestamp bigint,\n",
    "    name character varying(20),\n",
    "    version integer DEFAULT 1\n",
    ")\n",
    "DISTSTYLE EVEN;''']\n",
    "db_util.create_database_tables(sql_statements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded CSV file to s3://felixh-demo/etl-data/data_anomaly1.csv\n",
      "Successfully uploaded CSV file to s3://felixh-demo/etl-data/data_anomaly0.csv\n",
      "Successfully uploaded CSV file to s3://felixh-demo/etl-data/data_anomaly2.csv\n",
      "Successfully uploaded CSV file to s3://felixh-demo/etl-data/data_anomaly3.csv\n"
     ]
    }
   ],
   "source": [
    "# upload all etl csv files to S3 bucket\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "prefix = 'etl-data'\n",
    "\n",
    "\n",
    "# create prefix 'transforms' in S3 bucket\n",
    "s3.put_object(Bucket=S3_BUCKET_NAME, Key=f'{prefix}/')\n",
    "\n",
    "# upload all files from glue-component to s3_path\n",
    "for file in os.listdir('syn-data'):\n",
    "    try:\n",
    "        s3.upload_file(\n",
    "            f'syn-data/{file}',  # Local file path\n",
    "            S3_BUCKET_NAME,               # S3 bucket name\n",
    "            f'{prefix}/{file}'         # S3 key (path in bucket)\n",
    "        )\n",
    "        print(f\"Successfully uploaded CSV file to s3://{S3_BUCKET_NAME}/{prefix}/{file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading CSV file to S3: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  count\n",
      "0  None\n"
     ]
    }
   ],
   "source": [
    "# test Redshift connection and query table\n",
    "sql_statement = 'SELECT COUNT(*) FROM public.syn_data'\n",
    "result = db_util.run_sql(sql_statement)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded updated Glue job script to s3://felixh-demo/glue/glue-etl-processing.py\n",
      "S3 script path: s3://felixh-demo/glue/glue-etl-processing.py\n"
     ]
    }
   ],
   "source": [
    "# create AWS Glue Redshift ETL job script\n",
    "\n",
    "glue_job_script = f'''import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DoubleType, ShortType\n",
    "import logging\n",
    "import boto3\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize the Glue context\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "\n",
    "# Get job parameters\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'input_path', 'output_path', 'redshift_connection', 'redshift_table'])\n",
    "input_path = args['input_path']\n",
    "output_path = args['output_path']\n",
    "redshift_connection = args['redshift_connection']\n",
    "redshift_table = args['redshift_table']\n",
    "\n",
    "logger.info(f'Input path: {{input_path}}')\n",
    "logger.info(f'Redshift connection: {{redshift_connection}}')\n",
    "logger.info(f'Redshift table: {{redshift_table}}')\n",
    "\n",
    "\n",
    "# Set up the job\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "def get_redshift_connection_details(connection_name):\n",
    "    glue_client = boto3.client('glue')\n",
    "    try:\n",
    "        response = glue_client.get_connection(Name=connection_name)\n",
    "        connection_properties = response['Connection']['ConnectionProperties']\n",
    "        \n",
    "        return {{\n",
    "            'jdbc_url': connection_properties['JDBC_CONNECTION_URL'],\n",
    "            'username': connection_properties['USERNAME'],\n",
    "            'password': connection_properties['PASSWORD']\n",
    "        }}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting Redshift connection details: {{e}}\")\n",
    "        raise\n",
    "\n",
    "def get_redshift_table_schema(connection_details, table_name):\n",
    "    try:\n",
    "        # Use Spark to query Redshift\n",
    "        df = spark.read.format(\"jdbc\").option(\"url\", connection_details['jdbc_url']).option(\"dbtable\", f\"information_schema.columns\").option(\"user\", connection_details['username']).option(\"password\", connection_details['password']).option(\"driver\", \"com.amazon.redshift.jdbc42.Driver\").load()\n",
    "        \n",
    "        # Filter for the specific table and get column information\n",
    "        columns_info = df.filter((df.table_schema == table_name.split('.')[0]) & \n",
    "                                 (df.table_name == table_name.split('.')[1])).select(\"column_name\", \"data_type\").collect()\n",
    "        \n",
    "        # Create a schema based on Redshift data types\n",
    "        schema = StructType()\n",
    "        for col_info in columns_info:\n",
    "            col_name = col_info['column_name']\n",
    "            data_type = col_info['data_type']\n",
    "            if data_type == 'integer':\n",
    "                schema.add(StructField(col_name, IntegerType()))\n",
    "            elif data_type == 'bigint':\n",
    "                schema.add(StructField(col_name, LongType()))\n",
    "            elif data_type in ['double precision', 'real']:\n",
    "                schema.add(StructField(col_name, DoubleType()))\n",
    "            elif data_type == 'smallint':\n",
    "                schema.add(StructField(col_name, ShortType()))\n",
    "            else:\n",
    "                schema.add(StructField(col_name, StringType()))\n",
    "        \n",
    "        return schema\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting Redshift table schema: {{e}}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    # Get Redshift connection details\n",
    "    connection_details = get_redshift_connection_details(redshift_connection)\n",
    "    \n",
    "    # Get Redshift table schema\n",
    "    redshift_schema = get_redshift_table_schema(connection_details, redshift_table)\n",
    "    logger.info(f\"Redshift table schema: {{redshift_schema}}\")\n",
    "\n",
    "    # Read the CSV file from S3\n",
    "    logger.info(f\"Reading data from {{input_path}}\")\n",
    "    if not input_path.startswith('s3://'):\n",
    "        raise ValueError(f\"Invalid S3 path: {{input_path}}. Path must start with 's3://'\")\n",
    "    df = spark.read.csv(input_path, header=True, inferSchema=True)\n",
    "    \n",
    "    logger.info(f\"Read {{df.count()}} records from S3\")\n",
    "\n",
    "    # AWS GLUE DQ CHECKS\n",
    "    # https://docs.aws.amazon.com/glue/latest/dg/data-quality-gs-studio-notebooks.html\n",
    "    from awsgluedq.transforms import EvaluateDataQuality\n",
    "\n",
    "    EvaluateDataQuality_ruleset = \"\"\"\n",
    "        Rules = [\n",
    "            ColumnDataType \"id\" = \"Integer\",\n",
    "            ColumnDataType \"version\" = \"Integer\",\n",
    "            ColumnLength \"name\" <= 20\n",
    "            \n",
    "        ]\n",
    "        Analyzers = [\n",
    "        RowCount ,\n",
    "        ColumnCount ,\n",
    "        ColumnLength \"name\",\n",
    "        Completeness \"id\",\n",
    "        Completeness \"version\"\n",
    "        ]\n",
    "    \"\"\"\n",
    "\n",
    "    # convert dataframe to dynamic frame\n",
    "    dyf = DynamicFrame.fromDF(df, glueContext, \"dynamic_frame\")\n",
    "\n",
    "    EvaluateDataQualityMultiframe = EvaluateDataQuality().process_rows(\n",
    "    frame=dyf,\n",
    "    ruleset=EvaluateDataQuality_ruleset,\n",
    "    publishing_options={{\n",
    "            \"dataQualityEvaluationContext\": \"EvaluateDataQualityMultiframe\",\n",
    "            \"enableDataQualityCloudWatchMetrics\": False,\n",
    "            \"enableDataQualityResultsPublishing\": False\n",
    "        }},\n",
    "        additional_options={{\"performanceTuning.caching\": \"CACHE_NOTHING\"}}\n",
    "    )\n",
    "\n",
    "    # review results\n",
    "    ruleOutcomes = SelectFromCollection.apply(\n",
    "        dfc=EvaluateDataQualityMultiframe,\n",
    "        key=\"ruleOutcomes\",\n",
    "        transformation_ctx=\"ruleOutcomes\",\n",
    "    )\n",
    "\n",
    "    ruleOutcomes.toDF().show(truncate=False)\n",
    "\n",
    "    # review row level results\n",
    "    rowLevelOutcomes = SelectFromCollection.apply(\n",
    "        dfc=EvaluateDataQualityMultiframe,\n",
    "        key=\"rowLevelOutcomes\",\n",
    "        transformation_ctx=\"rowLevelOutcomes\",\n",
    "    )\n",
    "\n",
    "    rowLevelOutcomes_df = rowLevelOutcomes.toDF() # Convert Glue DynamicFrame to SparkSQL DataFrame\n",
    "    rowLevelOutcomes_df_passed = rowLevelOutcomes_df.filter(rowLevelOutcomes_df.DataQualityEvaluationResult == \"Passed\") # Filter only the Passed records.\n",
    "    rowLevelOutcomes_df.filter(rowLevelOutcomes_df.DataQualityEvaluationResult == \"Failed\").show(5, truncate=False) # Review the Failed records                    \n",
    "    rowLevelOutcomes_df_error = rowLevelOutcomes_df.filter(rowLevelOutcomes_df.DataQualityEvaluationResult == \"Failed\")\n",
    "\n",
    "    # write the Passed records to the destination. \n",
    "    # convert back to DynamicFrame before writing\n",
    "    rowLevelOutcomes_dyf_passed = DynamicFrame.fromDF(rowLevelOutcomes_df_passed, glueContext, \"passed_records\")\n",
    "    rowLevelOutcomes_dyf_error = DynamicFrame.fromDF(rowLevelOutcomes_df_error, glueContext, \"error_records\")\n",
    "    \n",
    "    # write error records to S3 destination for review\n",
    "    glueContext.write_dynamic_frame.from_options(\n",
    "        frame = rowLevelOutcomes_dyf_error,\n",
    "        connection_type = \"s3\",\n",
    "        connection_options = {{\"path\": f'{{output_path}}/etl_detected_dq_errors'}},\n",
    "        format = \"json\")\n",
    "         \n",
    "    # write records that passed Glue DQ checks to Amazon Redshift\n",
    "    original_columns = df.columns  # These are the columns from your input data\n",
    "    rowLevelOutcomes_df_passed_flat = rowLevelOutcomes_df_passed.select(original_columns)\n",
    "\n",
    "    df = rowLevelOutcomes_df_passed_flat\n",
    "\n",
    "    # align df schema with Redshift schema\n",
    "    for field in redshift_schema.fields:\n",
    "        if field.name in df.columns:\n",
    "            if isinstance(field.dataType, (IntegerType, LongType, ShortType)):\n",
    "                df = df.withColumn(field.name, col(field.name).cast(field.dataType))\n",
    "            elif isinstance(field.dataType, DoubleType):\n",
    "                df = df.withColumn(field.name, col(field.name).cast(DoubleType()))\n",
    "            else:\n",
    "                df = df.withColumn(field.name, col(field.name).cast(StringType()))\n",
    "        else:\n",
    "            df = df.withColumn(field.name, lit(None).cast(field.dataType))\n",
    "\n",
    "    # handle null values (optional, adjust as needed)\n",
    "    for column in df.columns:\n",
    "        df = df.withColumn(column, when(col(column) == \"\", None).otherwise(col(column)))\n",
    "    \n",
    "    # Convert back to DynamicFrame with only the original columns\n",
    "    dynamic_frame = DynamicFrame.fromDF(df, glueContext, \"flattened_passed_records\")\n",
    "\n",
    "    # Write to Redshift\n",
    "    logger.info(f\"Writing data to Redshift table {{redshift_table}}\")\n",
    "    glueContext.write_dynamic_frame.from_jdbc_conf(\n",
    "        frame=dynamic_frame,\n",
    "        catalog_connection=redshift_connection,\n",
    "        connection_options={{\n",
    "            \"dbtable\": redshift_table,\n",
    "            \"database\": connection_details['jdbc_url'].split('/')[-1]\n",
    "        }},\n",
    "        redshift_tmp_dir=f\"s3://{S3_BUCKET_NAME}/redshift-tmp/\",\n",
    "        transformation_ctx=\"datasink\"\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Data successfully written to Redshift\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred: {{str(e)}}\")\n",
    "    raise\n",
    "\n",
    "# Commit the job\n",
    "job.commit()\n",
    "'''\n",
    "\n",
    "# Upload the updated script to S3\n",
    "s3_client = boto3.client('s3')\n",
    "object_key = 'glue/glue-etl-processing.py'\n",
    "\n",
    "try:\n",
    "    s3_client.put_object(Bucket=S3_BUCKET_NAME, Key=object_key, Body=glue_job_script)\n",
    "    print(f\"Successfully uploaded updated Glue job script to s3://{S3_BUCKET_NAME}/{object_key}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading Glue job script to S3: {e}\")\n",
    "\n",
    "s3_script_path = f's3://{S3_BUCKET_NAME}/{object_key}'\n",
    "print(f\"S3 script path: {s3_script_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client = boto3.client('glue', region_name=REGION)\n",
    "\n",
    "glue_job_name = 'RedshiftETL-DQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create AWS Glue job via boto3\n",
    " \n",
    "response = glue_client.create_job(\n",
    "    Name=glue_job_name,\n",
    "    Description='ETL job to load data from S3 to Redshift',\n",
    "    Role=GLUE_IAM_ROLE_ARN,\n",
    "    ExecutionProperty={\n",
    "        'MaxConcurrentRuns': 1\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        '--connection-names': 'dev-redshift-connection'\n",
    "    },\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': s3_script_path,\n",
    "        'PythonVersion': '3'\n",
    "    },\n",
    "    MaxRetries=2,\n",
    "    Timeout=1440,\n",
    "    Tags={\n",
    "        'usecase': 'Glue ETL DQ'\n",
    "    },\n",
    "    GlueVersion='4.0',\n",
    "    NumberOfWorkers=1,\n",
    "    WorkerType='Standard',\n",
    "    Connections={\n",
    "        'Connections': ['dev-redshift-connection']\n",
    "    }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JobRunId': 'jr_d6ae499508d19f36e2b7b1bf05da785db70140d5e57aa161694859ef4d030e26', 'ResponseMetadata': {'RequestId': '66b1f961-ae04-4d32-802d-3400a67c971a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 13 Nov 2024 20:01:07 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '82', 'connection': 'keep-alive', 'x-amzn-requestid': '66b1f961-ae04-4d32-802d-3400a67c971a'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# run AWS Glue job with arguments\n",
    "\n",
    "args = {\n",
    "    '--JOB_NAME': glue_job_name,\n",
    "    '--input_path': f's3://felixh-demo/etl-data/data_anomaly0.csv',\n",
    "    '--output_path': f's3://felixh-demo/',\n",
    "    '--redshift_connection': 'dev-redshift-connection',\n",
    "    '--redshift_table': 'public.syn_data'\n",
    "}\n",
    "\n",
    "response = glue_client.start_job_run(\n",
    "    JobName=glue_job_name,\n",
    "    Arguments=args\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job run ID: jr_d44779cd138f335bc8d851a1651757e08fc9086527072dc1ebaf0c49b6acbc32\n"
     ]
    }
   ],
   "source": [
    "# run AWS Glue job again with different input file that has anomalies\n",
    "args = {\n",
    "    '--JOB_NAME': glue_job_name,\n",
    "    '--input_path': f's3://felixh-demo/etl-data/data_anomaly2.csv',\n",
    "    '--output_path': f's3://felixh-demo/',\n",
    "    '--redshift_connection': 'dev-redshift-connection',\n",
    "    '--redshift_table': 'public.syn_data'\n",
    "}\n",
    "\n",
    "response = glue_client.start_job_run(\n",
    "    JobName=glue_job_name,\n",
    "    Arguments=args\n",
    ")\n",
    "job_run_id = response['JobRunId']\n",
    "print(f'Job run ID: {job_run_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status: WAITING (attempt 1/30)\n",
      "Job status: RUNNING (attempt 2/30)\n",
      "Job status: RUNNING (attempt 3/30)\n",
      "Job status: RUNNING (attempt 4/30)\n",
      "Job status: RUNNING (attempt 5/30)\n",
      "Job status: RUNNING (attempt 6/30)\n",
      "Job status: SUCCEEDED (attempt 7/30)\n",
      "Final job status: SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "# wait on the status of the Glue job\n",
    "\n",
    "import time\n",
    "\n",
    "def wait_for_job_completion(job_name, run_id, max_attempts=30):\n",
    "    \"\"\"Wait for an AWS Glue job to complete, checking status every 30 seconds.\"\"\"\n",
    "    for i in range(max_attempts):\n",
    "        response = glue_client.get_job_run(JobName=job_name, RunId=run_id)\n",
    "        status = response['JobRun']['JobRunState']\n",
    "        \n",
    "        print(f\"Job status: {status} (attempt {i+1}/{max_attempts})\")\n",
    "        \n",
    "        if status in ['SUCCEEDED', 'FAILED', 'STOPPED', 'TIMEOUT']:\n",
    "            return status\n",
    "            \n",
    "        time.sleep(30)\n",
    "    \n",
    "    return 'TIMEOUT'\n",
    "\n",
    "# Wait for job completion\n",
    "final_status = wait_for_job_completion(glue_job_name, job_run_id)\n",
    "print(f\"Final job status: {final_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [id, timestamp, name, version]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# this last job should not have loaded any records to Redshift\n",
    "# because all of the rows had data quality issues\n",
    "\n",
    "# so let'scheck if any records were loaded to Redshift\n",
    "sql_statement = 'SELECT * FROM public.syn_data'\n",
    "result = db_util.run_sql(sql_statement)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook demonstrates how to use AWS Glue Data Quality to check for data quality issues in a CSV file and then load qualified records into an Amazon Redshift table."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
